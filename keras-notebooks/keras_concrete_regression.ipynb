{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data\n",
    "#Data can be downloaded here: https://cocl.us/concrete_data\n",
    "concrete = pd.read_csv('concrete_data.csv')\n",
    "concrete.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform EDA\n",
    "concrete.describe()\n",
    "\n",
    "#Then we can check how many values are null (aka any missing data points)\n",
    "concrete.isnull().sum()\n",
    "\n",
    "#No null values so we can proceed to sorting the data and building the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spliting Data into predictors and targets\n",
    "#Here we want to understand the taget variable, which in this case is Strength. Hence we will be want to use the following methodology to split the data\n",
    "concrete_columns = concrete.columns\n",
    "\n",
    "predictors = concrete[concrete_columns[concrete_columns != 'Strength']] \n",
    "target = concrete['Strength']\n",
    "\n",
    "#Now we want to normalize the data by subtracting the mean and divding by the standard deviation \n",
    "predictors_norm = (predictors - predictors.mean()) / predictors.std()\n",
    "predictors_norm.head()\n",
    "\n",
    "#Lastly we want to create a variable for the number of predictors as this will be used to enumerate the number of inputs\n",
    "n_cols = predictors_norm.shape[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and Run Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we will build the neural network by defining a function that builds the model\n",
    "def regression_model():\n",
    "    #Create the model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, activation='relu', input_shape = (n_cols,))) #We added one layer with 10 nodes with the input being the number of columns\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    #compile model\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a list where we will store all of the MSE's\n",
    "mse_list = []\n",
    "\n",
    "#Loop the function to repeat steps 1-3 50 times\n",
    "for i in range(50):\n",
    "\n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(predictors_norm, target, test_size=0.3, random_state=i)\n",
    "    # Build the model\n",
    "    model = regression_model()\n",
    "    # Train the model on the training set\n",
    "    model.fit(X_train, y_train, epochs=50, verbose=0)\n",
    "\n",
    "    #Model evaluation\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mse_list.append(mse)\n",
    "\n",
    "mean_mse = np.mean(mse_list)\n",
    "std_mse = np.std(mse_list)\n",
    "\n",
    "print(f\"Mean of the Mean Squared Errors over 50 runs is: {mean_mse}\")\n",
    "print(f\"Standard Deviation of the Mean Squared Errors over 50 runs is: {std_mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "A)Unnormalized data with 50 epochs:\n",
    "Mean of the Mean Squared Errors over 50 runs is: 840.7152468023643\n",
    "Standard Deviation of the Mean Squared Errors over 50 runs is: 2107.079341831513\n",
    "\n",
    "B)Normalized data with 50 epochs: \n",
    "Mean of the Mean Squared Errors over 50 runs is: 694.2724336560079\n",
    "Standard Deviation of the Mean Squared Errors over 50 runs is: 143.70527562917263\n",
    "\n",
    "The mean of the mean squared errors is lower than the unnormalized data which indicates that the model performs better. The standard deviation\n",
    "also decreased drasatically showcasing the more stable performance across runs. Normalizing the data improved performance. \n",
    "\n",
    "C)Normalized data with 100 epochs:\n",
    "Mean of the Mean Squared Errors over 50 runs is: 168.13926165692024\n",
    "Standard Deviation of the Mean Squared Errors over 50 runs is: 56.37702409980878\n",
    "\n",
    "The mean of the mean squared errors is signficantly lower comapred to part B, bringing it closer to the strength range in the data. Increasing\n",
    "training epochs allowed the model to learn more patterns in the data and improve predictive accuracy. More epochs also led to reduced error rates \n",
    "resulting the the smaller MSE and smaller STDEV. \n",
    "\n",
    "D)Normalized data with 50 epochs, three hidden layers, each of 10 nodes and ReLU activation function:\n",
    "Mean of the Mean Squared Errors over 50 runs is: 182.37070142096144\n",
    "Standard Deviation of the Mean Squared Errors over 50 runs is: 106.0576653820055\n",
    "\n",
    "Increasing the number of hidden layers from 1 to 3 yielded better performance compared to the model parameters in part B and this was done without\n",
    "the need to train with more epochs. \n",
    "\n",
    "Overall, the best performance was seen with the model parameters used in part C, suggesting training a single-layer model for more epochs was more\n",
    "beneficial then increasing the number of layers and keeping the number of epochs the same. This is most likely to do with the extra time alloted\n",
    "for the model to learn the data more effectively with a simpler architecture. \n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
